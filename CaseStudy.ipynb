{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0741cffb",
   "metadata": {},
   "source": [
    "# LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2202a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d2f99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text_data(file_path):\n",
    "    try:\n",
    "        return pd.read_csv(file_path, on_bad_lines='skip')\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load CSV: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb42c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert_model(model_name=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82993617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bert_model(model_name=\"bert-base-uncased\"):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b97b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(texts, tokenizer, model, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            # Ensure text is a string\n",
    "            text = str(text) if pd.notnull(text) else \"\"\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            mean_embedding = torch.mean(hidden_states, dim=1).squeeze().cpu().numpy()\n",
    "            embeddings.append(mean_embedding)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9ef44fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(image_path, size=(64, 64)):\n",
    "    if not os.path.exists(image_path):\n",
    "        return None\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert('RGB').resize(size)\n",
    "            img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "            return img_array\n",
    "    except IOError:\n",
    "        print(f\"Error opening or processing image {image_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4461c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator(text_file_path, image_folder_path, tokenizer, model, batch_size=32):\n",
    "    data = load_text_data(text_file_path)\n",
    "    if data is None or data.empty:\n",
    "        print(\"No data available or data loading failed.\")\n",
    "        return\n",
    "\n",
    "    current_year = data['year'].max()\n",
    "    popular_seasons = ['Summer', 'Winter']\n",
    "    data['y'] = ((data['year'] == current_year) & data['season'].isin(popular_seasons)).astype(int)\n",
    "\n",
    "    data['productDisplayName'] = data['productDisplayName'].astype(str)\n",
    "    text_features = get_bert_embeddings(data['productDisplayName'].tolist(), tokenizer, model)\n",
    "\n",
    "    batch_images = []\n",
    "    batch_text_features = []\n",
    "    batch_y = []\n",
    "    indices = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        image_path = os.path.join(image_folder_path, f\"{row['id']}.jpg\")\n",
    "        image_array = load_and_preprocess_image(image_path)\n",
    "        if image_array is not None:\n",
    "            batch_images.append(image_array)\n",
    "            batch_text_features.append(text_features[idx])\n",
    "            batch_y.append(row['y'])\n",
    "            indices.append(idx)\n",
    "            if len(batch_images) == batch_size:\n",
    "                yield np.array(batch_text_features), np.array(batch_images), np.array(batch_y)\n",
    "                batch_images, batch_text_features, batch_y = [], [], []\n",
    "\n",
    "    if batch_images:\n",
    "        yieldnp.array(batch_text_features), np.array(batch_images), np.array(batch_y)\n",
    "tokenizer, bert_model = load_bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e86a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r'C:\\Users\\John Justine\\Downloads\\archive (10)\\styles.csv'\n",
    "image_path = r'C:\\Users\\John Justine\\Downloads\\archive (10)\\e-commerce\\images'\n",
    "\n",
    "gen = dataset_generator(dataset_path, image_path, tokenizer, bert_model, batch_size=32)\n",
    "text_features, image_data_batch, y_batch = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecfda54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx]\n",
    "\n",
    "def extract_features_and_display_images(images, model_name=\"resnet50\", batch_size=32, device='cpu', num_images_to_display=6):\n",
    "    model = getattr(models, model_name)(pretrained=True)\n",
    "    model = nn.Sequential(*list(model.children())[:-1])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    transformed_images = [transform(image) for image in images]\n",
    "    dataset = CustomImageDataset(transformed_images)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    features = []\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    with torch.no_grad():\n",
    "        for batch_index, imgs in enumerate(loader):\n",
    "            print(f\"Processing batch {batch_index + 1}/{len(loader)}\") \n",
    "            imgs = imgs.to(device)\n",
    "            feats = model(imgs)\n",
    "            feats = feats.view(feats.size(0), -1)\n",
    "            features.append(feats.cpu().numpy())\n",
    "\n",
    "            if batch_index == 0: \n",
    "                for i, image in enumerate(imgs.cpu()):\n",
    "                    if i >= num_images_to_display:\n",
    "                        break\n",
    "                    plt.subplot(1, num_images_to_display, i + 1)\n",
    "                    plt.imshow(transforms.ToPILImage()(image))\n",
    "                    plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b38617",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_features = extract_features_and_display_images(image_data_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalFusionWithAttention(nn.Module):\n",
    "    def __init__(self, text_feature_dim, visual_feature_dim, fused_dim):\n",
    "        super().__init__()\n",
    "        self.text_weight = nn.Parameter(torch.randn(text_feature_dim, fused_dim))\n",
    "        self.visual_weight = nn.Parameter(torch.randn(visual_feature_dim, fused_dim))\n",
    "        self.attention_weight = nn.Parameter(torch.randn(fused_dim, 1))\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, text_features, visual_features):\n",
    "        text_transformed = torch.matmul(text_features, self.text_weight)\n",
    "        visual_transformed = torch.matmul(visual_features, self.visual_weight)\n",
    "        combined_features = self.activation(text_transformed + visual_transformed)\n",
    "        attention_scores = torch.matmul(combined_features, self.attention_weight)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1) \n",
    "        fused_features = combined_features * attention_weights.expand_as(combined_features)\n",
    "        return fused_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = torch.rand(32, 768)  \n",
    "visual_features = torch.rand(32, 2048)  \n",
    "\n",
    "fusion_model = MultimodalFusionWithAttention(768, 2048, 1024)\n",
    "fused_features = fusion_model(text_features.float(), visual_features.float())\n",
    "\n",
    "print(fused_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f525c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PurchaseIntentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "    \n",
    "        super(PurchaseIntentModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(1024, 256)  \n",
    "        self.bn1 = nn.BatchNorm1d(256)   \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)   \n",
    "        self.dropout = nn.Dropout(0.3)   \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)    \n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "model = PurchaseIntentModel()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b72ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, X_train, y_train, X_test, y_test, epochs=50):\n",
    "    model.train()  \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(X_train)  \n",
    "        loss = criterion(outputs.squeeze(), y_train)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        model.eval() \n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_test)\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_test)\n",
    "            val_losses.append(val_loss.item())\n",
    "        \n",
    "        model.train() \n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_losses, val_losses = train_model(model, criterion, optimizer, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X_test):\n",
    "    model.eval()  \n",
    "    with torch.no_grad():  \n",
    "        inputs = torch.tensor(X_test, dtype=torch.float32)\n",
    "        outputs = model(inputs)\n",
    "        predictions = outputs.squeeze().detach() \n",
    "    return predictions.numpy() \n",
    "\n",
    "predictions = predict(model, X_test)\n",
    "predictions = torch.sigmoid(torch.from_numpy(predictions)) \n",
    "predicted_labels = (predictions > 0.5).long() \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "predicted_labels = predicted_labels.numpy()\n",
    "y_test_array = y_test \n",
    "\n",
    "accuracy = accuracy_score(y_test_array, predicted_labels)\n",
    "precision = precision_score(y_test_array, predicted_labels)\n",
    "recall = recall_score(y_test_array, predicted_labels)\n",
    "f1 = f1_score(y_test_array, predicted_labels)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "average_precision = average_precision_score(y_test_array, predictions.numpy())\n",
    "\n",
    "print(f\"Mean Average Precision: {average_precision}\")\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "ndcg = ndcg_score(np.array([y_test_array]), np.array([predictions.numpy()]))\n",
    "\n",
    "print(f\"NDCG Score: {ndcg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689a095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bfb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the learned embeddings\n",
    "embeddings = model.embeddings.weight.data.numpy()\n",
    "target_names = np.unique(y_test_array)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, target_name in enumerate(target_names):\n",
    "    plt.scatter(embeddings[y_test_array == target_name, 0], \n",
    "                 embeddings[y_test_array == target_name, 1], \n",
    "                 label=target_name)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "def load_image(image_path, size=(224, 224)):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: The specified file does not exist - {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert('RGB').resize(size)\n",
    "            img_array = np.array(img)\n",
    "            img_array = img_array / 255.0\n",
    "\n",
    "            return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multimodal_recommendations(images, descriptions, scores, top_n=5):\n",
    "    fig = plt.figure(figsize=(15, 3 * top_n))\n",
    "    gs = gridspec.GridSpec(top_n, 2, width_ratios=[1, 3])\n",
    "\n",
    "    for i in range(top_n):\n",
    "        if images[i] is not None: \n",
    "            ax_image = plt.subplot(gs[i, 0])\n",
    "            ax_image.imshow(images[i])  \n",
    "            ax_image.axis('off')\n",
    "\n",
    "            ax_text = plt.subplot(gs[i, 1])\n",
    "            ax_text.text(0.5, 0.5, f'{descriptions[i]}\\nScore: {scores[i]:.2f}', \n",
    "                         verticalalignment='center', horizontalalignment='center',\n",
    "                         fontsize=12, multialignment='left')\n",
    "            ax_text.axis('off')\n",
    "        else:\n",
    "            print(f\"Failed to load image at index {i}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "image_paths = [\n",
    "    r'C:\\Users\\John Justine\\Downloads\\archive (10)\\e-commerce\\images\\8759.jpg',\n",
    "    r'C:\\Users\\John Justine\\Downloads\\archive (10)\\e-commerce\\images\\8760.jpg',\n",
    "    r'C:\\Users\\John Justine\\Downloads\\archive (10)\\e-commerce\\images\\8761.jpg',\n",
    "    r'C:\\Users\\John Justine\\Downloads\\archive (10)\\e-commerce\\images\\8762.jpg',\n",
    "    r'C:\\Users\\John Justine\\Downloads\\archive (10)\\e-commerce\\images\\8763.jpg'\n",
    "]\n",
    "\n",
    "sample_images = [load_image(path) for path in image_paths]\n",
    "\n",
    "sample_descriptions = [\n",
    "    \"Puma Men Grey T-shirt\",\n",
    "    \"Jealous 21 Women Purple Shirt\",\n",
    "    \"Rocky S Women White Handbag\",\n",
    "    \"Gini and Jony Boy's Kaleb White Brown Kidswear\",\n",
    "    \"Do U Speak Green Men Blue Shorts\"\n",
    "]\n",
    "sample_scores = [0.95, 0.92, 0.88, 0.86, 0.85]\n",
    "plot_multimodal_recommendations(sample_images, sample_descriptions, sample_scores, top_n=top_n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
